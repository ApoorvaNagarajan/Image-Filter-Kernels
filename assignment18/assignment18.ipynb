{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment18.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ApoorvaNagarajan/Image-Filter-Kernels/blob/master/assignment18/assignment18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRihSTgxAoWW",
        "colab_type": "text"
      },
      "source": [
        "#Importing all python packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMux2NjooUb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlFd6xBmoYC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SHUFFLENET_MEAN = [103.939, 116.779, 123.68]\n",
        "NORMALIZER = 0.017"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WLkG4smoHSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Shufflenet:\n",
        "    #Load pretrained model on initialization. Model downloaded from http://models.tensorpack.com/ImageNetModels/ShuffleNetV1-1x-g=8.npz\n",
        "    def __init__(self, model_loc):\n",
        "        self.trained_model = np.load(model_loc, encoding = 'latin1')\n",
        "        print(\"Pre-trained npz model loaded\")\n",
        "#       Uncomment below 2 lines to check model entries (Kernels for conv), (mean, variance, beta and gamma for BN) and (weights and biases) for final FC layer\n",
        "        for x in self.trained_model.files:\n",
        "            print(x + \" \" + str(self.trained_model[x].shape))\n",
        "\n",
        "    '''\n",
        "    Point-wise group convolution operation.\n",
        "    Inputs:     Activations of shape [N, H, W, C], Stage in format\n",
        "            'stagex', block in format 'blockx' and layer in\n",
        "            format 'convx', num_groups are the number of\n",
        "            groups to split the activations and kernels in.\n",
        "    Outputs:    Output activations of group convolution result\n",
        "    '''\n",
        "    def pw_gconv(self, activations, stage, block, layer, num_groups, name):\n",
        "        with tf.name_scope(name):\n",
        "            layer_name = str(stage) + '/' + str(block) + '/' + str(layer) + '/W:0'\n",
        "            kernels = self.trained_model[layer_name]\n",
        "            print(kernels.shape)\n",
        "            ch_per_group = activations.shape[3] // num_groups\n",
        "            act_split = tf.split(activations, num_or_size_splits = num_groups, axis = 3)\n",
        "            kernels_split = tf.split(kernels, num_or_size_splits = num_groups, axis = 3)\n",
        "            convs = []\n",
        "            for grp in range(0, num_groups):\n",
        "                convs.append(tf.nn.conv2d(act_split[grp], kernels_split[grp], padding = 'SAME', strides = [1, 1, 1, 1], data_format = 'NHWC', name='pw_gconv_' + str(grp)))\n",
        "            return tf.concat(convs, axis = 3)\n",
        "\n",
        "    '''\n",
        "    Depth-wise convolution operation.\n",
        "    Inputs:     Activations of shape [N, H, W, C], stage in format 'stagex',\n",
        "            block in format 'blockx', padding, stride and name to give to\n",
        "            the node in tensorboard visualization\n",
        "    Outputs:    Output activations of the dw conv operation\n",
        "    '''\n",
        "    def dw_conv(self, activations, stage, block, padding = 'SAME', stride = 1, name=\"dw_conv\"):\n",
        "        with tf.name_scope(name):\n",
        "            inp_ch = activations.shape[3]\n",
        "            act_shape = activations.shape\n",
        "            layer_name = str(stage) + '/' + str(block) + '/dconv/W:0'\n",
        "            kernels = self.trained_model[layer_name]\n",
        "            print(kernels.shape)\n",
        "            kernel_size = kernels.shape[0]\n",
        "            conv_result = tf.nn.depthwise_conv2d(activations, kernels, [1, stride, stride, 1], padding = padding, data_format = 'NHWC', name='dw_conv_' + stage + '_' + block)\n",
        "            return conv_result\n",
        "\n",
        "    '''\n",
        "    Batch Normalization operations\n",
        "    Inputs:     Activations of shape [N, H, W, C], stage in format 'stagex',\n",
        "            block in format 'blockx', layer in format 'convx' and name to\n",
        "            give to the node in tensorboard graph summary.\n",
        "    Outputs:    Output activations of BN operation\n",
        "    '''\n",
        "    def batch_normalization(self, activations, stage, block, layer, name):\n",
        "        with tf.name_scope(name):\n",
        "            layer_name = str(stage) + '/' + str(block) + '/' if stage is not '' else ''\n",
        "            layer_name = layer_name + 'conv1/bn/' if layer == 'conv1' else layer_name + layer+'_bn/'\n",
        "            bn_out = tf.nn.batch_normalization(activations, self.trained_model[layer_name + 'mean/EMA:0'], self.trained_model[layer_name + 'variance/EMA:0'], self.trained_model[layer_name + 'beta:0'], self.trained_model[layer_name + 'gamma:0'], variance_epsilon=0.001, name = 'bn_' + stage + '_' + block + '_' + layer if stage is not '' else 'bn_conv1')\n",
        "            return bn_out\n",
        "\n",
        "    '''\n",
        "    Channel Shuffle Operation. (Credits to Tensorpack Shufflenet Implementation https://github.com/tensorpack/tensorpack/blob/master/examples/ImageNetModels/shufflenet.py)\n",
        "    Inputs:     Activations of shape [N, H, W, C], num_groups = 8,\n",
        "            Name to give to the node in tensorboard graph summary.\n",
        "    Outputs     Activations after ch shuffle op.\n",
        "    '''\n",
        "    def channel_shuffle(self, activations, num_groups = 8, name='ch_shuffle'):\n",
        "        with tf.name_scope(name):\n",
        "            activations = tf.transpose(activations, perm = [0, 3, 1, 2])\n",
        "            in_shape = activations.get_shape().as_list()\n",
        "            in_channel = in_shape[1]\n",
        "            l = tf.reshape(activations, [-1, in_channel // num_groups, num_groups] + in_shape[-2:])\n",
        "            l = tf.transpose(l, [0, 2, 1, 3, 4])\n",
        "            l = tf.reshape(l, [-1, in_channel] + in_shape[-2:])\n",
        "            l = tf.transpose(l, perm = [0, 2, 3, 1])\n",
        "            return l\n",
        "\n",
        "    def shufflenet_unit(self, activations, stage, block, stride, num_groups=8, name=\"shufflenet_unit\"):\n",
        "        with tf.name_scope(name):\n",
        "            residual = activations\n",
        "            num_split = num_groups if activations.shape[3] > 24 else 1\n",
        "            print(\"Stage: \" + stage + \" block: \" + block)\n",
        "            print(\"inp_act size: \" + str(activations.shape))\n",
        "            pwgconv1 = self.pw_gconv(activations, stage, block, 'conv1', num_split, name= stage + \"_\" + block + \"_pwgconv1\")\n",
        "            print(\"pwgconv1 size: \" + str(pwgconv1.shape))\n",
        "            bnconv1 = self.batch_normalization(pwgconv1, stage, block, 'conv1', name = stage + \"_\" + block + \"_pwgconv1_batch_norm\")\n",
        "            reluconv1 = tf.nn.relu(bnconv1)\n",
        "            ch_sh = self.channel_shuffle(reluconv1, num_groups, name = stage + '_' + block + '_ch_shuffle')\n",
        "            dconv = self.dw_conv(ch_sh, stage, block, padding = 'SAME', stride = stride, name = stage + \"_\" + block + \"_dwconv\")\n",
        "            print(\"dconv size: \" + str(dconv.shape))\n",
        "            bndconv = self.batch_normalization(dconv, stage, block, 'dconv', name = stage + \"_\" + block + \"_dconv_batch_norm\")\n",
        "            pwgconv2 = self.pw_gconv(bndconv, stage, block, 'conv2', num_groups, name= stage + \"_\" + block + \"_pwgconv2\")\n",
        "            print(\"pwgconv2 size: \" + str(pwgconv2.shape))\n",
        "            bnconv2 = self.batch_normalization(pwgconv2, stage, block, 'conv2', name = stage + \"_\" + block + \"_pwgconv2_batch_norm\")\n",
        "\n",
        "            if stride == 1:\n",
        "                return tf.nn.relu(bnconv2 + residual, name = 'relu_' + stage + '_' + block)\n",
        "            elif stride == 2:\n",
        "                residual = tf.nn.avg_pool(residual, [1, 3, 3, 1], strides = [1, 2, 2, 1], padding = 'SAME', data_format = 'NHWC', name = 'avg_pool_' + stage + '_' + block)\n",
        "                return tf.concat([residual, tf.nn.relu(bnconv2)], axis = 3, name = 'concat_' + stage + '_' + block)\n",
        "            else:\n",
        "                raise ValueError(\"Stride value can only be 1 or 2 for Shufflenet\")\n",
        "\n",
        "    def shufflenet_stage(self, activations, stage, repeat, num_groups=8, name = \"shufflenet_stage\"):\n",
        "        with tf.name_scope(name):\n",
        "            first_block = self.shufflenet_unit(activations, stage, 'block0', stride = 2, num_groups = 8, name = \"shufflenet_unit_\" + stage + \"_block0\")\n",
        "            res = first_block\n",
        "            for b in range(1, repeat+1):\n",
        "                res = self.shufflenet_unit(res, stage, 'block' + str(b), stride = 1, num_groups = 8, name = \"shufflenet_unit_\" + stage + \"_block\" + str(b))\n",
        "            return res\n",
        "\n",
        "    def shufflenet_stage1(self, activations):\n",
        "        with tf.name_scope(\"shufflenet_stage1\"):\n",
        "            kernels = self.trained_model['conv1/W:0']\n",
        "            res = tf.nn.conv2d(activations, kernels, padding = 'SAME', strides = [1, 2, 2, 1], data_format = 'NHWC', name = 'Conv1')\n",
        "            res = self.batch_normalization(res, '', '', 'conv1', name = 'stage1_conv2d_batch_norm')\n",
        "            res = tf.nn.max_pool(res, [1, 3, 3, 1], strides = [1, 2, 2, 1], padding = 'SAME', data_format = 'NHWC', name = 'MaxPool1')\n",
        "            return res\n",
        "\n",
        "    def fc_layer(self, activations):\n",
        "        with tf.name_scope('fc_layer'):\n",
        "            layer_name = 'linear'\n",
        "            weights = self.trained_model[layer_name + '/W:0']\n",
        "            biases = self.trained_model[layer_name + '/b:0']\n",
        "            flattened_out = tf.contrib.layers.flatten(activations)\n",
        "            return tf.nn.bias_add(tf.matmul(flattened_out, weights), biases)\n",
        "\n",
        "    def build(self, image):\n",
        "        red, green, blue = tf.split(axis=3, num_or_size_splits=3, value=image)\n",
        "        bgr = tf.concat(axis=3, values=[(blue - SHUFFLENET_MEAN[0])*NORMALIZER, (green - SHUFFLENET_MEAN[1])*NORMALIZER, (red - SHUFFLENET_MEAN[2])*NORMALIZER])\n",
        "        stage1 = self.shufflenet_stage1(bgr)\n",
        "        stage2 = self.shufflenet_stage(stage1, 'stage2', repeat = 3, num_groups = 8, name = \"shufflenet_stage2\")\n",
        "        stage3 = self.shufflenet_stage(stage2, 'stage3', repeat = 7, num_groups = 8, name = \"shufflenet_stage3\")\n",
        "        stage4 = self.shufflenet_stage(stage3, 'stage4', repeat = 3, num_groups = 8, name = \"shufflenet_stage4\")\n",
        "        g_pool = tf.nn.avg_pool(stage4, [1, 7, 7, 1], strides = [1, 1, 1, 1], padding = 'VALID', data_format = 'NHWC', name = 'GlobalPool')\n",
        "        logits = self.fc_layer(g_pool)\n",
        "        logits = tf.nn.softmax(logits, name = \"SoftMax_unit\")\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}