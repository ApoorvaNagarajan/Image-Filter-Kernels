{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "end_game",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ApoorvaNagarajan/Image-Filter-Kernels/blob/master/p2s10/end_game.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpNVdfe2agWb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "ee169a67-28c4-4870-dced-37357d329b0d"
      },
      "source": [
        "!pip install -q torch==0.3.1 torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 496.4MB 37kB/s \n",
            "\u001b[31mERROR: torchvision 0.5.0 has requirement torch==1.4.0, but you'll have torch 0.3.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.60 has requirement torch>=1.0.0, but you'll have torch 0.3.1 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5Kwml8yXpHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get sand denities from the image\n",
        "\n",
        "# Importing the libraries\n",
        "seed=512\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image as PILImage\n",
        "import PIL\n",
        "import math\n",
        "from PIL import Image, ImageDraw\n",
        "from matplotlib import pyplot as plt \n",
        "import cv2\n",
        "from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE7ybR0LbUf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    bs_X1, bs_X2, next_bs_X1, next_bs_X2, batch_actions, batch_rewards, batch_dones = [], [], [], [], [], [], []\n",
        "    for i in ind: \n",
        "      state_X1, state_X2, next_state_X1, next_state_X2, action, reward, done = self.storage[i]\n",
        "      bs_X1.append(np.array(state_X1, copy=False))\n",
        "      bs_X2.append(np.array(state_X2, copy=False))\n",
        "      next_bs_X1.append(np.array(next_state_X1, copy=False))\n",
        "      next_bs_X2.append(np.array(next_state_X2, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(bs_X1), np.array(bs_X2), np.array(next_bs_X1), np.array(next_bs_X2), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIiQEHjYbZ61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 16, 3, stride=1, padding=(1,1))\n",
        "    self.conv1_bn = nn.BatchNorm2d(16)\n",
        "    self.conv2 = nn.Conv2d(16, 32, 3, stride=1, padding=(1,1))\n",
        "    self.conv2_bn = nn.BatchNorm2d(32)\n",
        "    self.conv3 = nn.Conv2d(32, 16, 3, stride=2, padding=(1,1))\n",
        "    self.conv3_bn = nn.BatchNorm2d(16)\n",
        "    self.conv4 = nn.Conv2d(16, 10, 3, stride=2, padding=(1,1))\n",
        "    self.conv4_bn = nn.BatchNorm2d(10)\n",
        "    self.fc1 = nn.Linear(14, 256)\n",
        "    #self.fc1_bn = nn.BatchNorm1d(256)\n",
        "    self.fc2 = nn.Linear(256, 128)\n",
        "    #self.fc2_bn = nn.BatchNorm1d(128)\n",
        "    self.fc3 = nn.Linear(128, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x1, x2):                # input x1: 32x32x1, x2=3\n",
        "    x1 = x1.cuda()\n",
        "    x2 = x2.cuda()\n",
        "    h = F.relu(self.conv1_bn(self.conv1(x1)))              # 32x32x16\n",
        "    h = F.relu(self.conv2_bn(self.conv2(h)))               # 32x32x32\n",
        "    h = F.relu(self.conv3_bn(self.conv3(h)))               # 16x16x16\n",
        "    h = F.relu(self.conv4_bn(self.conv4(h)))               # 8x8x10\n",
        "    h = F.avg_pool2d(h, h.size()[2:])       # 10\n",
        "    h = h.view(-1, 10)\n",
        "    h = torch.cat([h, x2], dim=1)\n",
        "    h = F.relu(self.fc1(h))\n",
        "    h = F.relu(self.fc2(h))\n",
        "    h = self.max_action * torch.tanh(self.fc3(h))\n",
        "    return h\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.conv1 = nn.Conv2d(1, 16, 3, stride=1, padding=(1,1))\n",
        "    self.conv1_bn = nn.BatchNorm2d(16)\n",
        "    self.conv2 = nn.Conv2d(16, 32, 3, stride=1, padding=(1,1))\n",
        "    self.conv2_bn = nn.BatchNorm2d(32)\n",
        "    self.conv3 = nn.Conv2d(32, 16, 3, stride=2, padding=(1,1))\n",
        "    self.conv3_bn = nn.BatchNorm2d(16)\n",
        "    self.conv4 = nn.Conv2d(16, 10, 3, stride=2, padding=(1,1))\n",
        "    self.conv4_bn = nn.BatchNorm2d(10)\n",
        "    self.fc1 = nn.Linear(14 + action_dim, 256)\n",
        "    #self.fc1_bn = nn.BatchNorm1d(256)\n",
        "    self.fc2 = nn.Linear(256, 128)\n",
        "    #self.fc2_bn = nn.BatchNorm1d(128)\n",
        "    self.fc3 = nn.Linear(128, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.conv5 = nn.Conv2d(1, 16, 3, stride=1, padding=(1,1))\n",
        "    self.conv5_bn = nn.BatchNorm2d(16)\n",
        "    self.conv6 = nn.Conv2d(16, 32, 3, stride=1, padding=(1,1))\n",
        "    self.conv6_bn = nn.BatchNorm2d(32)\n",
        "    self.conv7 = nn.Conv2d(32, 16, 3, stride=2, padding=(1,1))\n",
        "    self.conv7_bn = nn.BatchNorm2d(16)\n",
        "    self.conv8 = nn.Conv2d(16, 10, 3, stride=2, padding=(1,1))\n",
        "    self.conv8_bn = nn.BatchNorm2d(10)\n",
        "    self.fc4 = nn.Linear(14 + action_dim, 256)\n",
        "    #self.fc4_bn = nn.BatchNorm1d(256)\n",
        "    self.fc5 = nn.Linear(256, 128)\n",
        "    #self.fc5_bn = nn.BatchNorm1d(128)\n",
        "    self.fc6 = nn.Linear(128, 1)\n",
        "\n",
        "  def forward(self, x1, x2, u):\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = x1.cuda()\n",
        "    x2 = x2.cuda()\n",
        "    u = u.cuda()\n",
        "    h1 = F.relu(self.conv1_bn(self.conv1(x1)))               # 32x32x16\n",
        "    h1 = F.relu(self.conv2_bn(self.conv2(h1)))               # 32x32x32\n",
        "    h1 = F.relu(self.conv3_bn(self.conv3(h1)))               # 16x16x16\n",
        "    h1 = F.relu(self.conv4_bn(self.conv4(h1)))               # 8x8x10\n",
        "    h1 = F.avg_pool2d(h1, h1.size()[2:])       # 10\n",
        "    h1 = h1.view(-1, 10)\n",
        "    h1 = torch.cat([h1, x2], dim=1)\n",
        "    h1 = torch.cat([h1, u], dim=1)\n",
        "    h1 = F.relu(self.fc1(h1))\n",
        "    h1 = F.relu(self.fc2(h1))\n",
        "    h1 = self.fc3(h1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    h2 = F.relu(self.conv5_bn(self.conv5(x1)))              # 32x32x16\n",
        "    h2 = F.relu(self.conv6_bn(self.conv6(h2)))              # 32x32x32\n",
        "    h2 = F.relu(self.conv7_bn(self.conv7(h2)))               # 16x16x16\n",
        "    h2 = F.relu(self.conv8_bn(self.conv8(h2)))               # 8x8x10\n",
        "    h2 = F.avg_pool2d(h2, h2.size()[2:])       # 10\n",
        "    h2 = h2.view(-1, 10)\n",
        "    h2 = torch.cat([h2, x2], dim=1)\n",
        "    h2 = torch.cat([h2, u], dim=1)\n",
        "    h2 = F.relu(self.fc4(h2))\n",
        "    h2 = F.relu(self.fc5(h2))\n",
        "    h2 = self.fc6(h2)\n",
        "    return h1, h2\n",
        "\n",
        "  def Q1(self, x1,x2, u):\n",
        "    x1 = x1.cuda()\n",
        "    x2 = x2.cuda()\n",
        "    u = u.cuda()\n",
        "    h1 = F.relu(self.conv1_bn(self.conv1(x1)))              # 32x32x16\n",
        "    h1 = F.relu(self.conv2_bn(self.conv2(h1)))               # 32x32x32\n",
        "    h1 = F.relu(self.conv3_bn(self.conv3(h1)))               # 16x16x16\n",
        "    h1 = F.relu(self.conv4_bn(self.conv4(h1)))               # 8x8x10\n",
        "    h1 = F.avg_pool2d(h1, h1.size()[2:])       # 10\n",
        "    h1 = h1.view(-1, 10)\n",
        "    h1 = torch.cat([h1, x2], dim=1)\n",
        "    h1 = torch.cat([h1, u], dim=1)\n",
        "    h1 = F.relu(self.fc1(h1))\n",
        "    h1 = F.relu(self.fc2(h1))\n",
        "    h1 = self.fc3(h1)\n",
        "    return h1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52oRNevRbmE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action)\n",
        "    self.actor.cuda()\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_target.cuda()\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), weight_decay=0.01)\n",
        "    self.critic = Critic(state_dim, action_dim)\n",
        "    self.critic.cuda()\n",
        "    self.critic_target = Critic(state_dim, action_dim)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_target.cuda()\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), weight_decay=0.01)\n",
        "    self.max_action = max_action\n",
        "    self.replay_buffer = ReplayBuffer()\n",
        "    self.total_timesteps = 0\n",
        "    self.episode_reward = 0\n",
        "    self.episode_num = 0\n",
        "    self.episode_timesteps = 0\n",
        "\n",
        "  def select_action(self, X1, X2):\n",
        "        print(X2)\n",
        "        if(self.total_timesteps < start_timesteps):\n",
        "            print(\"random action \", self.total_timesteps)\n",
        "            return np.random.randint(-5,5, size=1)\n",
        "        else:\n",
        "            print(\"nw action \", self.total_timesteps)\n",
        "            X1 = torch.Tensor(X1.reshape(1, 1, 32, 32))\n",
        "            X2 = torch.Tensor(np.asarray(X2).reshape(1, -1))\n",
        "            return self.actor(Variable(X1, volatile = True), Variable(X2, volatile = True))\n",
        "\n",
        "  def train(self, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      bs_X1, bs_X2, next_bs_X1, next_bs_X2, batch_actions, batch_rewards, batch_dones = self.replay_buffer.sample(batch_size)\n",
        "      X1 = Variable(torch.Tensor(bs_X1), volatile = False)\n",
        "      X2 = Variable(torch.Tensor(bs_X2), volatile = False)\n",
        "      next_X1 = Variable(torch.Tensor(next_bs_X1), volatile = False)\n",
        "      next_X2 = Variable(torch.Tensor(next_bs_X2), volatile = False)\n",
        "      action = Variable(torch.Tensor(batch_actions), volatile = False)\n",
        "      reward = Variable(torch.Tensor(batch_rewards), volatile = True)\n",
        "      done = Variable(torch.Tensor(batch_dones), volatile = True)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_X1, next_X2)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = Variable(torch.Tensor(batch_actions), volatile = True).data.normal_(0, policy_noise)\n",
        "      noise = Variable(noise.clamp(-noise_clip, noise_clip), volatile = True)\n",
        "      next_action = (next_action + noise.cuda()).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_X1, next_X2, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2).cuda()\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward.cuda() + ((1 - done.cuda()) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(X1, X2, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      #print(\"critic loss \",critic_loss)\n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(X1, X2, self.actor(X1, X2)).mean()\n",
        "        #print(\"actor_loss \",actor_loss)\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\t\t  \n",
        "    \n",
        "  def add_replay_buff(self, X1, X2, new_X1, new_X2, action, reward, done_flag):\n",
        "        self.episode_reward += reward\n",
        "        # if reward is lesser than min reward, end the episode\n",
        "        if(self.episode_reward<min_episode_reward):\n",
        "            done_flag = 1\n",
        "        self.replay_buffer.add((X1, X2, new_X1, new_X2, action, reward, done_flag))\n",
        "        self.total_timesteps += 1\n",
        "        self.episode_timesteps += 1\n",
        "        # If episode is done, train the model\n",
        "        if (done_flag == 1):\n",
        "            print(self.episode_num, \" : EPISODE REWARD \", self.episode_reward)\n",
        "            self.train(self.episode_timesteps)\n",
        "            self.episode_reward = 0\n",
        "            self.episode_num += 1\n",
        "            self.episode_timesteps = 0\n",
        "        return done_flag\n",
        "\n",
        "  # Making a save method to save a trained model\n",
        "  def save(self):\n",
        "    torch.save(self.actor.state_dict(), 'last_actor.pth')\n",
        "    torch.save(self.critic.state_dict(), 'last_critic.pth')\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    if os.path.isfile('last_actor.pth'):\n",
        "        self.actor.load_state_dict(torch.load('last_actor.pth'))\n",
        "    if os.path.isfile('last_critic.pth'):\n",
        "        self.critic.load_state_dict(torch.load('last_critic.pth' ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcCoTH5Kb2YX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init():\n",
        "    global sand\n",
        "    global img\n",
        "    global goal_x\n",
        "    global goal_y\n",
        "    global first_update\n",
        "    global map_width\n",
        "    global map_height\n",
        "\n",
        "    map_width = 1429\n",
        "    map_height = 660\n",
        "    \n",
        "    # Read the mask image\n",
        "    sand = np.zeros((map_height,map_width))\n",
        "    img = cv2.imread(\"MASK1.png\",0) \n",
        "    sand = img/255\n",
        "          \n",
        "    goal_x = 1197\n",
        "    goal_y = 512\n",
        "    first_update = False\n",
        "    global swap\n",
        "    swap = 0\n",
        "    global done_flag\n",
        "    done_flag = 0\n",
        "    global total_timesteps\n",
        "    total_timesteps = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWOdV4x4mJiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Car(object):\n",
        "    \n",
        "    def __init__(self):\n",
        "      self.angle = 0.0\n",
        "      self.rotation = 0.0\n",
        "      self.velocity_x = 0.0\n",
        "      self.velocity_y = 0.0\n",
        "      self.pos_x = 0.0\n",
        "      self.pos_y = 0.0\n",
        "\n",
        "    def move(self, rotation):\n",
        "      \n",
        "        self.pos_x = self.velocity_x + self.pos_x\n",
        "        self.pos_y = self.velocity_y + self.pos_y\n",
        "        self.rotation = float(rotation)\n",
        "        self.angle = (self.angle + self.rotation)%360\n",
        "        \n",
        "    def reset(self):\n",
        "        print(\"RESETTING\")\n",
        "        self.pos_x = np.random.randint(80, map_width-80, size=1)\n",
        "        self.pos_y = np.random.randint(80, map_height-80, size=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEYUBoybiHFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rotate(vector_x, vector_y, angle):\n",
        "  return (vector_x * math.cos(angle)) - (vector_y * math.sin(angle)), (vector_y * math.cos(angle)) + (vector_x * math.sin(angle))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIwuki55iLdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_angle(vector1_x, vector1_y, vector2_x, vector2_y):\n",
        "  angle = -(180 / math.pi) * math.atan2(vector1_x * vector2_y - vector1_y * vector2_x, vector1_x * vector2_x + vector1_y * vector2_y)\n",
        "  return angle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc32-XEynB9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Game(object):\n",
        "\n",
        "    def __init__(self):\n",
        "      self.car = Car()\n",
        "      self.goal_x = 0\n",
        "      self.goal_y = 0\n",
        "      init()\n",
        "      self.surr = self.get_surroundings()\n",
        "\n",
        "    def serve_car(self):\n",
        "        #self.car.center = self.center\n",
        "        self.car.velocity_x = 6\n",
        "        self.car.velocity_y = 0\n",
        "        self.car.pos_x = map_width/2\n",
        "        self.car.pos_y = map_height/2\n",
        "\n",
        "        \n",
        "    def get_surroundings(self):\n",
        "        \n",
        "        crop_img = sand[map_height-1-int(self.car.pos_y)-crop_size: map_height-1- int(self.car.pos_y)+crop_size, int(self.car.pos_x)-crop_size:int(self.car.pos_x)+crop_size].copy()\n",
        "       \n",
        "        top = 0\n",
        "        bottom = 0\n",
        "        left = 0\n",
        "        right = 0\n",
        "         \n",
        "        # if at frame boundary, pad the cropped image with sand (1's)\n",
        "        if(crop_img.shape[0] != 2*crop_size): # rows\n",
        "            if(self.car.pos_y < crop_size):\n",
        "                bottom = 2*crop_size - crop_img.shape[0]\n",
        "            else:\n",
        "                top = 2*crop_size - crop_img.shape[0]\n",
        "            \n",
        "        if(crop_img.shape[1] != 2*crop_size): # colums\n",
        "            if(self.car.pos_x < crop_size):\n",
        "                left = 2*crop_size - crop_img.shape[1]\n",
        "            else:\n",
        "                right = 2*crop_size - crop_img.shape[1]            \n",
        "            \n",
        "        if((top != 0) or (bottom != 0) or (left != 0) or (right != 0)):\n",
        "            crop_img = cv2.copyMakeBorder(crop_img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=1 )    \n",
        "        #cv2.imshow(\"crop_img\",crop_img)\n",
        "        #cv2.waitKey(0)  \n",
        "\n",
        "        pt1 = rotate(30, 0, -self.car.angle) + (crop_size, crop_size)\n",
        "        pt2 = rotate(0, 10, -self.car.angle) + (crop_size, crop_size)\n",
        "        pt3 = rotate(0, -10, -self.car.angle) + (crop_size, crop_size)\n",
        "        triangle_cnt = np.array( [pt1, pt2, pt3] )\n",
        "        ctr = np.array(triangle_cnt).reshape((-1,3,2)).astype(np.int32)\n",
        "        cv2.fillPoly(crop_img, pts =ctr, color=0.5)     \n",
        "        #cv2.imshow(\"Car\",crop_img)\n",
        "        #cv2.waitKey(0) \n",
        "        \n",
        "        rsz_img = cv2.resize(crop_img, (32,32), interpolation = cv2.INTER_AREA)\n",
        "        #cv2.imshow(\"resized_image\",rsz_img)\n",
        "        #cv2.waitKey(0) \n",
        "        \n",
        "        rsz_img = rsz_img.reshape(1, 32, 32)\n",
        "\n",
        "        return rsz_img\n",
        "\n",
        "    def update(self):\n",
        "\n",
        "        global brain\n",
        "        global last_reward\n",
        "        global scores\n",
        "        global last_distance\n",
        "        global goal_x\n",
        "        global goal_y\n",
        "        global map_width\n",
        "        global map_height\n",
        "        global swap\n",
        "        global done_flag\n",
        "        global total_timesteps\n",
        "        \n",
        "        xx = goal_x - self.car.pos_x\n",
        "        yy = goal_y - self.car.pos_y\n",
        "        orientation = get_angle(self.car.velocity_x, self.car.velocity_y, xx, yy)/180.\n",
        "        \n",
        "        \n",
        "        # states : \n",
        "        #32x32 cropped image with car overlay\n",
        "        #orientation\n",
        "        #-orientation\n",
        "        #distance_x from goal\n",
        "        #distance_y from goal       \n",
        "        X1 = self.surr       \n",
        "        X2 = [orientation, -orientation, xx, yy]\n",
        "\n",
        "        # actions:\n",
        "        # angle theta of rotation       \n",
        "        action = brain.select_action(X1, X2)\n",
        "        print(action)\n",
        "\n",
        "        #if np.isnan(action[0]):\n",
        "        #  action[0] = 0\n",
        "        #  done_flag = 1\n",
        "\n",
        "        self.car.move(action[0]) \n",
        "        on_road = 0\n",
        "\n",
        "        if self.car.pos_x < border_size:\n",
        "            self.car.pos_x = border_size\n",
        "            last_reward = -10\n",
        "            done_flag = 1\n",
        "        if self.car.pos_x > map_width - border_size:\n",
        "            self.car.pos_x = map_width - border_size\n",
        "            last_reward = -10\n",
        "            done_flag = 1\n",
        "        if self.car.pos_y < border_size:\n",
        "            self.car.pos_y = border_size\n",
        "            last_reward = -10\n",
        "            done_flag = 1\n",
        "        if self.car.pos_y > map_height - border_size:\n",
        "            self.car.pos_y = map_height - border_size\n",
        "            last_reward = -10\n",
        "            done_flag = 1\n",
        "\n",
        "        if(0 == done_flag):\n",
        "        \n",
        "            # velocity\n",
        "            if sand[map_height-1-int(self.car.pos_y), int(self.car.pos_x)] > 0:\n",
        "                self.car.velocity_x, self.car.velocity_y = rotate(0.5, 0, self.car.angle)\n",
        "                on_road = 0\n",
        "                print(\"SAND\")\n",
        "            else: # otherwise\n",
        "                self.car.velocity_x, self.car.velocity_y = rotate(2, 0, self.car.angle)\n",
        "                on_road = 1\n",
        "                print(\"ROAD\")\n",
        "            \n",
        "            new_xx = goal_x - self.car.pos_x\n",
        "            new_yy = goal_y - self.car.pos_y\n",
        "            new_orient = get_angle(self.car.velocity_x, self.car.velocity_y, new_xx, new_yy)/180.\n",
        "            new_X1 = self.get_surroundings()\n",
        "            new_X2 = [new_orient, -new_orient, new_xx, new_yy]\n",
        "            self.surr = new_X1\n",
        "\n",
        "            # Rewards\n",
        "            distance = np.sqrt((self.car.pos_x - goal_x)**2 + (self.car.pos_y - goal_y)**2)\n",
        "\n",
        "            if((on_road == 1) and (distance < last_distance)):\n",
        "                last_reward = 1\n",
        "            elif((on_road == 0) and (distance < last_distance)):\n",
        "                last_reward = -2\n",
        "            elif((on_road == 1) and (distance > last_distance)):\n",
        "                last_reward = -1\n",
        "            elif((on_road == 0) and (distance > last_distance)):\n",
        "                last_reward = -4\n",
        "        else:  \n",
        "\n",
        "            # Rewards\n",
        "            distance = np.sqrt((self.car.pos_x - goal_x)**2 + (self.car.pos_y - goal_y)**2)\n",
        "            new_X1 = X1\n",
        "            new_X2 = X2\n",
        "\n",
        "        if distance < 25:\n",
        "            print(\"GOALLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLL REACHEDDDDDDDDDDDDDDDDDD\")\n",
        "            if swap == 1:\n",
        "                goal_x = 1197\n",
        "                goal_y = 512\n",
        "                swap = 0\n",
        "                done_flag = 1\n",
        "            else:\n",
        "                goal_x = 361\n",
        "                goal_y = 311\n",
        "                swap = 1\n",
        "                done_flag = 1\n",
        "                \n",
        "        last_distance = distance\n",
        "\n",
        "        done_flag = brain.add_replay_buff(X1, X2, new_X1, new_X2, action, last_reward, done_flag)\n",
        "\n",
        "        if(done_flag == 1):\n",
        "            self.car.reset()\n",
        "            self.surr = self.get_surroundings()\n",
        "            done_flag = 0\n",
        "\n",
        "        total_timesteps += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98MbvJZmbT4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_timesteps = 1e5 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated\t\n",
        "min_episode_reward = -30000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9QhZh6cchyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "first_update = True\n",
        "crop_size = 80\n",
        "border_size = 5\n",
        "brain = TD3(5,1,5)\n",
        "last_distance = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU5UT4AIdkMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "init()\n",
        "game = Game()\n",
        "game.serve_car()\n",
        "max_num_timesteps = 500000\n",
        "timesteps = 0\n",
        "\n",
        "while (1):\n",
        "  game.update()\n",
        "  timesteps += 1\n",
        "  if(timesteps >= max_num_timesteps):\n",
        "    break\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}