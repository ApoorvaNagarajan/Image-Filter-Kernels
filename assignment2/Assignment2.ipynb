{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ApoorvaNagarajan/Image-Filter-Kernels/blob/master/assignment2/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nJ7YEw_vyjG",
        "colab_type": "text"
      },
      "source": [
        "# **Not an ideal network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGWv5hBhv2jf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c21cbc77-43fd-4a66-8c22-fcd2be696d77"
      },
      "source": [
        "# https://keras.io/\n",
        "# import keras which is a deep learning python library. Keras is capable of\n",
        "# running on top of most of the opensource deep learning frameworks and makes \n",
        "# our life very easy\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnMlDJQKv4VG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# numpy is scientific computing python package. This also provides us \n",
        "# multi-dimensional array functionality. We will need this to represent \n",
        "# our data and for some computation\n",
        "import numpy as np\n",
        "\n",
        "# importing keras core data structure packages needed to build our network\n",
        "\n",
        "# A model is a way to organize layers. Sequential model means a linear stack of layers\n",
        "from keras.models import Sequential\n",
        "# Import Flatten and Convolution2D layers which we will use to build our network\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Convolution2D\n",
        "# Import keras np_utils which provide some utilities for the numpy objects \n",
        "# that people generally use while building networks\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# MNIST database is a large database of 10 hand written digits (0 to 9). \n",
        "# It provides both testing and training images. We will use mnist dataset for\n",
        "# training our network\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CdSu2lMwB9s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0a87d917-1b2d-4ccf-fc65-b7aa657940fd"
      },
      "source": [
        "# load_data function downloads the MNIST dataset and splits it into test and train.\n",
        "# It returns 2 tuples containing test and train data\n",
        "# X_train is an array of images used for training\n",
        "# Y_train is an array of digit labels indicating the digits from 0 to 9 for the images in X_train\n",
        "# X_test is an array of images used for testing\n",
        "# Y_test is an array of digit labels indicating the digits from 0 to 9 for the images in X_test. This is used to validate our network output\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLaDf0-rwCmj",
        "colab_type": "code",
        "outputId": "a5fee943-28ac-4984-9e36-a843c37225d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "# print the dimensions of the X_train array. From the output looks like there\n",
        "# are 60000 images of size 28x28\n",
        "print (X_train.shape)\n",
        "\n",
        "# import pyton plot libraries which help us visualize data\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Visualize 1st image of the training set\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f4c01b68d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erb11jNwwFwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_train and X_test are reshaped from 3 dimensions to 4 dimensions\n",
        "# 1st dim : num images\n",
        "# 2nd and 3rd dim : Width and height of eaach of the images\n",
        "# 4th dim : number of channels in each of the image. We set this to 1 as the\n",
        "#           our dataset has only gray images\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLK4YDoRwHet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We are normalizing our test and train images. This will make values of all\n",
        "# the pixels lie between 0 to 1. Normalization helps to maintain accuracy \n",
        "# even after a huge number of multiplications involved in multiple convolutions\n",
        "# First converting the datatype to float to support storing fractional values\n",
        "# resulting from normalization\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNKLOmhlwJQl",
        "colab_type": "code",
        "outputId": "f5e3813b-d4ad-4e7e-a65c-b0c44ec70abc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# print the first 10 traing image labels\n",
        "y_train[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YusMJguiwKsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "# 1-dimensional class arrays store only the class \n",
        "# class matricies store the probability of the image for each of the classes\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upxc99AswMW0",
        "colab_type": "code",
        "outputId": "e2b54a5d-3a38-49bb-d85a-067eb2cc8096",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "# print the class matrix for easy understanding of the class matrix\n",
        "Y_train[:10]\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irTVUE47wNwr",
        "colab_type": "code",
        "outputId": "8ff9f666-c537-4fc7-e490-c4a8ee453dfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        }
      },
      "source": [
        "# This code defines our network\n",
        "\n",
        "# import libraries for Activation and MaxPooling2D layers from keras\n",
        "from keras.layers import Activation, MaxPooling2D\n",
        "\n",
        "# Our model is a sequential model. We are going to stack layers one after the other\n",
        "model = Sequential() \n",
        "# 1st layer : convolution layer, input ch 1, 32 3x3x1 kernels, receptive field 3x3, output 26x26x32\n",
        "model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
        "# 2nd layer : convolution layer, input ch 32, 64 3x3x32 kernels, receptive field 5x5, output 24x24x64\n",
        "model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
        "# 3rd layer : convolution layer, input ch 64, 128 3x3x64 kernels, receptive field 7x7, output 22x22x128\n",
        "model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
        "\n",
        "# 4th layer : max pooling layer, input ch 128, 2x2 max pooling operation, receptive field 14x14, output 11x11x128\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# 5th layer : convolution layer, input ch 128, 256 3x3x128 kernels, receptive field 16x16, output 9x9x256\n",
        "model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
        "# 6th layer : convolution layer, input ch 256, 512 3x3x256 kernels, receptive field 18x18, output 7x7x512\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
        "# 7th layer : convolution layer, input ch 512, 1024 3x3x512 kernels, receptive field 20x20, output 5x5x1024\n",
        "model.add(Convolution2D(1024, 3, 3, activation='relu'))\n",
        "# 8th layer : convolution layer, input ch 1024, 2048 3x3x1024 kernels, receptive field 22x22, output 3x3x2048\n",
        "model.add(Convolution2D(2048, 3, 3, activation='relu'))\n",
        "# 9th layer : convolution layer, input ch 2048, 10 3x3x2048 kernels, receptive field 24x24, output 1x1x10\n",
        "model.add(Convolution2D(10, 3, 3, activation='relu'))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "# softmax layer is added just before the output layer. This converts the network\n",
        "# analysis to probabilities for each of the image classes\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# prints the summary of the network just built\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1024, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2048, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 22, 22, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 9, 9, 256)         295168    \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 7, 7, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 5, 5, 1024)        4719616   \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 3, 3, 2048)        18876416  \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 1, 1, 10)          184330    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 25,348,362\n",
            "Trainable params: 25,348,362\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYZOpRb6yG7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the network that we have put together. We are definig the \n",
        "# loss as categorical_crossentropy\n",
        "# optimization method as adam (adaptive estimates of lower-order moments)\n",
        "# ADAM is a first order gradient descent based optimization strategy\n",
        "# Metric is used to evaluate the performace of the model. We are using accuracy as the metric\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O248wVQyMft",
        "colab_type": "code",
        "outputId": "a5b3a5d8-98cf-4ebc-ed01-f9568aaaaaea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "# Training function. This function filts a predictor to the training\n",
        "# data provided. To do that, we are configuring a batch size of 32 and number of epoch to 10.\n",
        "# So this network will be run 10 times for backpropagation to correct the weights\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=10, verbose=1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 119s 2ms/step - loss: 1.4419 - acc: 0.4026\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 114s 2ms/step - loss: 1.3873 - acc: 0.4110\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 113s 2ms/step - loss: 1.3738 - acc: 0.4131\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 113s 2ms/step - loss: 1.3707 - acc: 0.4130\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 113s 2ms/step - loss: 1.3679 - acc: 0.4137\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 113s 2ms/step - loss: 1.3685 - acc: 0.4134\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 113s 2ms/step - loss: 1.3672 - acc: 0.4129\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 113s 2ms/step - loss: 1.3692 - acc: 0.4128\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 113s 2ms/step - loss: 1.3712 - acc: 0.4129\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 113s 2ms/step - loss: 1.3661 - acc: 0.4132\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4bfe96f668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sst4KneiyOL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate the fitted predictor and compare with the ground truth to give us a metric score\n",
        "# Returns 2 values\n",
        "# 1st loss values\n",
        "# 2nd metric value. we have configured metric to accuracy. So we will get back accuracy\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfJiXOKsyj4y",
        "colab_type": "code",
        "outputId": "f50c7f6d-85f6-4f88-fecd-f7730a347546",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# print the returned score. Outut indicates that the network is very bad and inaccurate\n",
        "print(score)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.3695639165878295, 0.4096]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwLSXt7nyn_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate the predicted output for the test images\n",
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWKKoOKwyppN",
        "colab_type": "code",
        "outputId": "f05bcd4f-313c-4e4a-b235-ea9326874af2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "# Just print first 9 predicted values and ground truth values to visualize how\n",
        "# inaccurate the network is\n",
        "print(y_pred[:9])\n",
        "print(y_test[:9])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.9179991e-09 1.9179991e-09 1.9179991e-09 1.9179991e-09 1.9179991e-09\n",
            "  1.9179991e-09 1.9179991e-09 1.0000000e+00 1.9179991e-09 1.9179991e-09]\n",
            " [1.0000000e-01 1.0000000e-01 1.0000000e-01 1.0000000e-01 1.0000000e-01\n",
            "  1.0000000e-01 1.0000000e-01 1.0000000e-01 1.0000000e-01 1.0000000e-01]\n",
            " [1.1480945e-11 1.0000000e+00 1.1480945e-11 1.1480945e-11 1.1480945e-11\n",
            "  1.1480945e-11 1.1480945e-11 1.1480945e-11 1.1480945e-11 1.1480945e-11]\n",
            " [1.0000000e+00 5.0974786e-15 5.0974786e-15 5.0974786e-15 5.0974786e-15\n",
            "  5.0974786e-15 5.0974786e-15 5.0974786e-15 5.0974786e-15 5.0974786e-15]\n",
            " [1.0000000e-01 1.0000000e-01 1.0000000e-01 1.0000000e-01 1.0000000e-01\n",
            "  1.0000000e-01 1.0000000e-01 1.0000000e-01 1.0000000e-01 1.0000000e-01]\n",
            " [3.5240612e-13 1.0000000e+00 3.5240612e-13 3.5240612e-13 3.5240612e-13\n",
            "  3.5240612e-13 3.5240612e-13 3.5240612e-13 3.5240612e-13 3.5240612e-13]\n",
            " [1.0000000e-01 1.0000000e-01 1.0000000e-01 1.0000000e-01 1.0000000e-01\n",
            "  1.0000000e-01 1.0000000e-01 1.0000000e-01 1.0000000e-01 1.0000000e-01]\n",
            " [1.0000000e-01 1.0000000e-01 1.0000000e-01 1.0000000e-01 1.0000000e-01\n",
            "  1.0000000e-01 1.0000000e-01 1.0000000e-01 1.0000000e-01 1.0000000e-01]\n",
            " [1.0000000e-01 1.0000000e-01 1.0000000e-01 1.0000000e-01 1.0000000e-01\n",
            "  1.0000000e-01 1.0000000e-01 1.0000000e-01 1.0000000e-01 1.0000000e-01]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCR0HLssI-RJ",
        "colab_type": "text"
      },
      "source": [
        "**Issue with the above network**\n",
        "\n",
        "The problem with this network is, we are not filtering out unnecessary data after the max pooling layer (4th layer). At the 9th layer we have accumulated too much unnecessary information or features (2048 channels). From this 2048 channels, we try to collate to 10 channels which should correspond to our ten classes. This makes the network very inaccurate. We should rather try and reduce channels towards the end of the network\n",
        "\n",
        "Receptive field of the network seems to be ok as the network has seen almost entire image. \n",
        "\n",
        "In the below cell I have corrected the code to make the model better"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u6EuJe_LVFX",
        "colab_type": "code",
        "outputId": "781761f6-23b1-4993-b086-36cf241a4c61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1582
        }
      },
      "source": [
        "# Our model is a sequential model. We are going to stack layers one after the other\n",
        "model_new = Sequential() \n",
        "# 1st layer : convolution layer, input ch 1, 32 3x3x1 kernels, receptive field 3x3, output 26x26x32\n",
        "model_new.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
        "# 2nd layer : convolution layer, input ch 32, 64 3x3x32 kernels, receptive field 5x5, output 24x24x64\n",
        "model_new.add(Convolution2D(64, 3, 3, activation='relu'))\n",
        "# 3rd layer : convolution layer, input ch 64, 128 3x3x64 kernels, receptive field 7x7, output 22x22x128\n",
        "model_new.add(Convolution2D(128, 3, 3, activation='relu'))\n",
        "\n",
        "# 4th layer : max pooling layer, input ch 128, 2x2 max pooling operation, receptive field 14x14, output 11x11x128\n",
        "model_new.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# 5th layer : convolution layer, input ch 128, 32 3x3x128 kernels, receptive field 16x16, output 9x9x32\n",
        "model_new.add(Convolution2D(256, 3, 3, activation='relu'))\n",
        "# 6th layer : convolution layer, input ch 32, 64 3x3x32 kernels, receptive field 18x18, output 7x7x64\n",
        "model_new.add(Convolution2D(128, 3, 3, activation='relu'))\n",
        "# 7th layer : convolution layer, input ch 64, 128 3x3x64 kernels, receptive field 20x20, output 5x5x128\n",
        "model_new.add(Convolution2D(64, 3, 3, activation='relu'))\n",
        "# 8th layer : convolution layer, input ch 128, 64 3x3x128 kernels, receptive field 22x22, output 3x3x64\n",
        "model_new.add(Convolution2D(32, 3, 3, activation='relu'))\n",
        "# 9th layer : convolution layer, input ch 64, 10 3x3x2048 kernels, receptive field 24x24, output 1x1x10\n",
        "model_new.add(Convolution2D(10, 3, 3, activation='relu'))\n",
        "\n",
        "model_new.add(Flatten())\n",
        "\n",
        "# softmax layer is added just before the output layer. This converts the network\n",
        "# analysis to probabilities for each of the image classes\n",
        "model_new.add(Activation('softmax'))\n",
        "\n",
        "# prints the summary of the network just built\n",
        "model_new.summary()\n",
        "\n",
        "\n",
        "# Compile the network that we have put together. We are definig the \n",
        "# loss as categorical_crossentropy\n",
        "# optimization method as adam (adaptive estimates of lower-order moments)\n",
        "# ADAM is a first order gradient descent based optimization strategy\n",
        "# Metric is used to evaluate the performace of the model. We are using accuracy as the metric\n",
        "model_new.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "# Run the model just compiled. This function filts a predictor to the training\n",
        "# data provided\n",
        "model_new.fit(X_train, Y_train, batch_size=32, nb_epoch=10, verbose=1)\n",
        "\n",
        "# Evaluate the fitted predictor and compare with the ground truth to give us a metric score\n",
        "# Returns 2 values\n",
        "# 1st loss values\n",
        "# 2nd metric value. we have configured metric to accuracy. So we will get back accuracy\n",
        "score = model_new.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "# print the returned score. Outut indicates that the network is very bad and inaccurate\n",
        "print(score)\n",
        "\n",
        "# Generate the predicted output for the test images\n",
        "y_pred = model_new.predict(X_test)\n",
        "\n",
        "# Just print first 9 predicted values and ground truth values to visualize how\n",
        "# inaccurate the network is\n",
        "print(y_pred[:9])\n",
        "print(y_test[:9])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_65 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_66 (Conv2D)           (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_67 (Conv2D)           (None, 22, 22, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_68 (Conv2D)           (None, 9, 9, 256)         295168    \n",
            "_________________________________________________________________\n",
            "conv2d_69 (Conv2D)           (None, 7, 7, 128)         295040    \n",
            "_________________________________________________________________\n",
            "conv2d_70 (Conv2D)           (None, 5, 5, 64)          73792     \n",
            "_________________________________________________________________\n",
            "conv2d_71 (Conv2D)           (None, 3, 3, 32)          18464     \n",
            "_________________________________________________________________\n",
            "conv2d_72 (Conv2D)           (None, 1, 1, 10)          2890      \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 778,026\n",
            "Trainable params: 778,026\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 18s 308us/step - loss: 0.3507 - acc: 0.8718\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 17s 287us/step - loss: 0.2569 - acc: 0.8970\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 17s 288us/step - loss: 0.2460 - acc: 0.8999\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 18s 292us/step - loss: 0.1607 - acc: 0.9361\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 17s 290us/step - loss: 0.0280 - acc: 0.9919\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 17s 285us/step - loss: 0.0223 - acc: 0.9934\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 17s 285us/step - loss: 0.0208 - acc: 0.9938\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.0184 - acc: 0.9945\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 17s 291us/step - loss: 0.0143 - acc: 0.9960\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.0161 - acc: 0.9950\n",
            "[0.028991936376044577, 0.9927]\n",
            "[[7.65132124e-10 7.65132124e-10 5.75615688e-09 7.65132124e-10\n",
            "  7.65132124e-10 7.65132124e-10 7.65132124e-10 1.00000000e+00\n",
            "  7.65132124e-10 7.65132124e-10]\n",
            " [2.80581331e-13 3.56591623e-12 1.00000000e+00 2.80581331e-13\n",
            "  1.88615833e-12 2.80581331e-13 2.80581331e-13 3.40672521e-12\n",
            "  6.33893762e-13 2.80581331e-13]\n",
            " [1.77105971e-06 9.99954462e-01 1.77105971e-06 1.77105971e-06\n",
            "  2.05059314e-06 7.23688481e-06 1.77105971e-06 1.93723008e-05\n",
            "  8.00873659e-06 1.77105971e-06]\n",
            " [9.99988317e-01 1.12095137e-07 1.12095137e-07 1.12095137e-07\n",
            "  2.44541667e-07 1.26675116e-06 8.88757950e-06 1.12095137e-07\n",
            "  3.53286055e-07 5.08085122e-07]\n",
            " [5.11575894e-12 5.11575894e-12 5.11575894e-12 5.11575894e-12\n",
            "  1.00000000e+00 5.11575894e-12 3.42886053e-10 5.11575894e-12\n",
            "  8.37711150e-12 2.64765265e-10]\n",
            " [1.09814700e-07 9.99996066e-01 1.09814700e-07 1.09814700e-07\n",
            "  1.09814700e-07 3.27782374e-07 1.09814700e-07 2.74898048e-06\n",
            "  1.75450438e-07 1.09814700e-07]\n",
            " [1.21286607e-08 5.79285597e-07 1.21286607e-08 1.21286607e-08\n",
            "  9.99999046e-01 1.21286607e-08 1.21286607e-08 1.21286607e-08\n",
            "  3.25522961e-07 4.33837322e-08]\n",
            " [2.91841351e-09 2.91841351e-09 2.91841351e-09 2.91841351e-09\n",
            "  1.01474086e-07 2.91841351e-09 2.91841351e-09 2.91841351e-09\n",
            "  3.49820439e-09 9.99999881e-01]\n",
            " [9.36849483e-06 9.36849483e-06 9.36849483e-06 9.36849483e-06\n",
            "  9.36849483e-06 9.98693645e-01 1.14142522e-03 9.36849483e-06\n",
            "  9.93174181e-05 9.36849483e-06]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}